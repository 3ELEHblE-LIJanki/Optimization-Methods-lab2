{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346272c5-b2d3-4bac-9a74-42ff24b2df4b",
   "metadata": {},
   "source": [
    "В дополнение к реализованному нами методу рассмотрим реализацию из библиотеки `scipy.optimize`. Для замеров её эффективности и построения графиков будем использовать ScipyWrapper написанный в прошлой лабораторной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e4788-d46f-402e-9c3d-1bd4fd9685e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./lib\")\n",
    "from lrs import *\n",
    "from gradient_descent import ScipyWrapper\n",
    "from newton import NewtonCG\n",
    "from function_wrapper import FunctionWrapper\n",
    "from output import pretty_print\n",
    "from graphics_plotter import GraphicsPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43d34c-fc48-45e4-bf24-266a8f3c099a",
   "metadata": {},
   "source": [
    "Для начала рассмотрим простую функцию и сравним как работает на ней библиотечный метод, а как реализованный нами.\n",
    "$(x - 2)^2 + (y - 4)^2 + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3955b-3118-4c94-93a9-28610fe4e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda: FunctionWrapper(lambda x: (x[0] - 2)**2 + (x[1] - 4)**2 + 1)\n",
    "bounds = [[-5, 5], [-5, 5]]\n",
    "start = [-2, 2]\n",
    "max_iter = 1000\n",
    "\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "newton = NewtonCG(constant(1), func(), bounds)\n",
    "res = newton.find_min(start, max_iter)\n",
    "pretty_print(newton, \"NEWTON\", res, gradient, hessian) \n",
    "\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "f = func()\n",
    "wrapper = ScipyWrapper(f, bounds)\n",
    "res = wrapper.find_min(\"Newton-CG\", start, max_iter, lambda x: gradient(f, x, 1e-5), lambda x: hessian(f, x, 1e-5))\n",
    "pretty_print(wrapper, \"Scipy\", res, gradient, hessian)\n",
    "\n",
    "plotter1 = GraphicsPlotter(newton)\n",
    "plotter1.plot(\"Our Newton\")\n",
    "plotter2 = GraphicsPlotter(wrapper)\n",
    "plotter2.plot(\"Scipy-Newton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c860a-992b-46ba-979a-596d9e2c867b",
   "metadata": {},
   "source": [
    "Теперь посмотрим на интересную функцию из 1го задания \"Банановая долина\". $(1 - x)^2 + 100(y-x^2)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b1cf5-704d-407e-a049-aba64429f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda: FunctionWrapper(lambda x: (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2)\n",
    "\n",
    "bounds = [[-5, 5], [-5, 5]]\n",
    "start = [-2, 2]\n",
    "max_iter = 1000\n",
    "\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "f = func()\n",
    "wrapper = ScipyWrapper(f, bounds)\n",
    "res = wrapper.find_min(\"Newton-CG\", start, max_iter, lambda x: gradient(f, x, 1e-5), lambda x: hessian(f, x, 1e-5))\n",
    "pretty_print(wrapper, \"Scipy\", res, gradient, hessian)\n",
    "\n",
    "plotter2 = GraphicsPlotter(wrapper)\n",
    "plotter2.plot(\"Scipy-Newton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24094f76-448c-424e-b276-11adc13db7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [1, 0.5, 0.1]:\n",
    "    gradient.clear()\n",
    "    hessian.clear()\n",
    "    newton = NewtonCG(constant(c), func(), bounds)\n",
    "    plotter1 = GraphicsPlotter(newton)\n",
    "    res = newton.find_min(start, max_iter)\n",
    "    pretty_print(newton, \"NEWTON\", res, gradient, hessian) \n",
    "    plotter1.plot(\"Our Newton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5398432-4f5d-4cda-adda-52b1b50ebb50",
   "metadata": {},
   "source": [
    "По данным графикам отлично видно, что наш метод повторяет библиотечную реализацию, кроме выбора шага. В зависимости от выбранной длины шага наш метод делает разное кол-во шагов и лишь при шаге длиной 0.1 от изначальной, мы приблизились по кол-ву итераций к библиотечной реализации. При этом на предыдущем примере методы отработали примерно одинаково - это наводит на мысль, что библиотечная реализация использует динамический метод выбора шага, мы с ними тоже поэксперементировали в 1 задании, но ничего лучше 1 на функциях, выбранных нами, найти не удалось."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e03ee6-4860-4b7d-9b56-ef362fe8facc",
   "metadata": {},
   "source": [
    "Теперь посмотрим на реализованный в библиотеке метод \"BFGS\". Его преимущество в том, что он не вычисляет обратный Гессиан, а пересчитывает его итеративно, получая ассимптотику $O(n^2)$ на итерацию. Начнём также с простой квадратичной функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a1783-e76f-4356-b716-63fe9f3574e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda: FunctionWrapper(lambda x: (x[0] - 2)**2 + (x[1] - 4)**2 + 1)\n",
    "bounds = [[-5, 5], [-5, 5]]\n",
    "start = [-2, 2]\n",
    "max_iter = 1000\n",
    "\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "f = func()\n",
    "wrapper = ScipyWrapper(f, bounds)\n",
    "res = wrapper.find_min(\"BFGS\", start, max_iter, lambda x: gradient(f, x, 1e-5))\n",
    "pretty_print(wrapper, \"Scipy\", res, gradient)\n",
    "\n",
    "plotter2 = GraphicsPlotter(wrapper)\n",
    "plotter2.plot(\"Scipy-BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586b89a-b47f-48eb-a1b3-bcacdf40d05f",
   "metadata": {},
   "source": [
    "Как видно, он тоже отлично доходит до минимальной точки, но гораздо меньше раз вызвал функцию при том же кол-ве шагов т.к. не считал Гессианы. Теперь посмотрим на банановую долину, на которой библиотечная реализация метода Ньютона работала достаточно долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8d90f-d4f1-4ce7-83fb-7ce96cb2581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda: FunctionWrapper(lambda x: (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2)\n",
    "\n",
    "bounds = [[-5, 5], [-5, 5]]\n",
    "start = [-2, 2]\n",
    "max_iter = 1000\n",
    "\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "f = func()\n",
    "wrapper = ScipyWrapper(f, bounds)\n",
    "res = wrapper.find_min(\"BFGS\", start, max_iter, lambda x: gradient(f, x, 1e-5))\n",
    "pretty_print(wrapper, \"Scipy\", res, gradient)\n",
    "\n",
    "plotter2 = GraphicsPlotter(wrapper)\n",
    "plotter2.plot(\"Scipy-BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc1ce0-4558-47bc-acf8-ec99f3e2111f",
   "metadata": {},
   "source": [
    "А вот здесь уже отличия разительные. BFGS - сделал в 4 раза меньше шагов, соответственно гораздо меньше раз посчитал функцию, не говоря о том, что каждая итерация была в 2 раза быстрее чем у метода Ньютона. Давайте посмотрим как ведёт себя данный метод на мультимодальных функциях (вдруг он лишён и этого недостатка метода Ньютона) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786ff61-ce56-4aa3-8c64-9af5ac024127",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_sin = lambda c: FunctionWrapper(lambda x: np.sin(c * x[0]) + (x[0] + x[1])**2 + x[0]**2 + x[1]**2)\n",
    "\n",
    "bounds = [[-2.5, 2.5], [-2.5, 2.5]]\n",
    "start = [-2, -2]\n",
    "max_iter = 1000\n",
    "\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "f = func_sin(5)\n",
    "wrapper = ScipyWrapper(f, bounds)\n",
    "res = wrapper.find_min(\"BFGS\", start, max_iter, lambda x: gradient(f, x, 1e-5))\n",
    "pretty_print(wrapper, \"Scipy\", res, gradient)\n",
    "\n",
    "plotter2 = GraphicsPlotter(wrapper)\n",
    "plotter2.plot(\"Scipy-BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c234d0-a8b0-418e-92fb-13c438f72566",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter2.plot_3d(\"Scipy-BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a481b-afe8-4c09-9d4d-1c081100fc32",
   "metadata": {},
   "source": [
    "Вывод: BFGS - из библиотеки scipy.optimize - это улучшенная версия метода Ньютона. Она лишена одного из недостатков - работает кратно быстрее, однако под капотом делает тоже самое, что и метод Ньютона - ищет точку, в которой градиент равен 0 и поэтому всё ещё не работает на мультимодальных функциях."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
