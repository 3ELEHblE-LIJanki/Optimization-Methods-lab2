{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2\n",
    "## Продвинутые методы\n",
    "***\n",
    "Авторы: Заречнев Алексей, Петрасюк Алексей, Халили Алина, Галимова Ярослава (все - счастливые обитатели группы M3236)  \n",
    "[Ссылка на репозиторий](https://github.com/3ELEHblE-LIJanki/Optimization-Methods-lab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе нами был реализован метод Ньютона. Его можно найти в файле `newton.py` в репозитории [`lib`](https://github.com/3ELEHblE-LIJanki/Optimization-lib/tree/08f159d6b4ff8b11ee9a361be8e92b438ea8a6f8), на который ведёт сабмодуль [этого](https://github.com/3ELEHblE-LIJanki/Optimization-Methods-lab2) репозитория. Главная цель прежняя — найти точку минимума функции. Только теперь мы оперируем методом **второго порядка**, то есть нас интересует вторая производная функции. Здесь тоже всё как раньше - мы используем уже реализованные нами вспомогательные методы из `lrs.py` и классы для рисования графиков из прошлой лабораторной работы. В чём глобальное отличие?  \n",
    "\n",
    "Если простыми словами, то градиентный спуск говорит: «Иди вниз по склону по направлению наискорейшего убывания функции», в то время как метод Ньютона несколько умнее: он не только смотрит на уклон, но и учитывает то, как быстро меняется уклон. Это напоминает стратегии выбора шага Вольфе или Гольдштейна, которые проверяли походи условия, но при этом опирались только на градиент (то есть являлись методами первого порядка). Давайте посмотрим, как это работает на практике и сравним результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Метод Ньютона с выбором шага и с одномерным поиском"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Ньютона использует вторые производные (гессиан) для учета кривизны функции. Он аппроксимирует функцию квадратичной формой и находит минимум этой аппроксимации. А как мы умеем аппроксимировать? По Тейлору!  \n",
    "$f(x_k + p) = f(x_k) + grad(f(x_k)) * p + 1/2 * p^T * H(f(x_k))*p$  \n",
    "$H$ - гессиан (иначе - матрица вторых производных или, как мы обозначали на матане, $d^2f$)  \n",
    "\n",
    "Тогда следующий шаг будет вычисляться как $x_{k+1} = x_k + l * H^(-1)*grad(f(x_k))$  \n",
    "Мы же используем функцию `boundize`, чтобы оставаться в заданных рамках, а так же в нашем случае есть `l` - learning_rate_shedule, которая по умолчанию равна 1 (в классическом методе Ньютона), но мы также можем адаптировать наш метод и превратить его в **damped Newton method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Сравнение с библиотекой `scipy.optimize`. Метод Newton-CG и квазиньютоновские методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Квазиньютоновский метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Библиотека `optuna` и её применение на функциях"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
