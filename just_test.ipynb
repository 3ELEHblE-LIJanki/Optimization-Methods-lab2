{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./lib\")\n",
    "from graphics_plotter import *\n",
    "from output import *\n",
    "from lrs import *\n",
    "from linear_algo import golden_ratio, dichotomy\n",
    "from gradient_descent import *\n",
    "from newton import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f13560841317e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearAll(f, g, h):\n",
    "    f.clear()\n",
    "    g.clear()\n",
    "    h.clear()\n",
    "\n",
    "# тестирование ф-ии из библиотеки scipy.optimize\n",
    "func = FunctionWrapper(lambda x: 6 * x[0] ** 2 + 4 * x[1] * x[1] - 3 * x[1] )\n",
    "\n",
    "x0 = [-6.0, 12.0]\n",
    "max_steps = 10000\n",
    "\n",
    "grad = FunctionWrapper(lambda x: gradient(func, x, 1e-4))\n",
    "hes = FunctionWrapper(lambda x: hessian(func, x, 1e-4))\n",
    "\n",
    "\n",
    "optimize_CG = ScipyWrapper(func, [[-6.0, 12.0]], 0.001)\n",
    "\n",
    "# Newton-CG (метод Ньютона с сопряжёнными градиентами)\n",
    "# Точный метод Ньютона.\tОн использует гессиан <- который мы его и передаем. (наш, написанный) + Градиент передаем.\n",
    "resNewtone = optimize_CG.find_min('Newton-CG', x0, max_steps, gradient=grad, hessian=hes)\n",
    "pretty_print(optimize_CG, 'Newton-CG', resNewtone, grad, hes)\n",
    "\n",
    "\n",
    "# BFGS\t- Квазиньютоновский. Он как то аппроксимирует гессиан, не требует его явного вычисления. -> передаем только градиент\n",
    "# думаю разобраться в его коде - и можно использовать в доп задании\n",
    "\n",
    "clearAll(func, grad, hes)\n",
    "resBFGS = optimize_CG.find_min('BFGS', x0, max_steps, gradient=grad)\n",
    "pretty_print(optimize_CG, 'BFGS', resBFGS, grad, hes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bea7e40b71794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfgs import BFGS\n",
    "from lrs import gradient\n",
    "from output import pretty_print\n",
    "from graphics_plotter import GraphicsPlotter\n",
    "\n",
    "func = FunctionWrapper(lambda x: 6 * x[0] ** 2 + 4 * x[1] * x[1] - 3 * x[1])\n",
    "\n",
    "x0 = [-6.0, 12.0]\n",
    "max_steps = 10000\n",
    "\n",
    "bfgs = BFGS(func, [[-10, 20], [-10, 20]])\n",
    "\n",
    "gradient.clear()\n",
    "res = bfgs.find_min(x0, max_steps)\n",
    "pretty_print(bfgs, \"BFGS\", res, gradient)\n",
    "\n",
    "plotter = GraphicsPlotter(bfgs)\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62ca16c1e27d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b338557e7dcc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=\"test\", direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c143da96f66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = FunctionWrapper(lambda x: x[0] ** 2 + 3 * x[1] ** 2 + x[1] * x[0] - 1)\n",
    "test_descent = GradientDecent(wolfe(0.0001, 0.9), func, [[-10.0, 10.0], [-10.0, 10.0]], 0.00001)\n",
    "\n",
    "gradient.clear()\n",
    "result = test_descent.find_min(tuple([3, 3]), 1200)\n",
    "test_plotter = GraphicsPlotter(test_descent)\n",
    "\n",
    "pretty_print(test_descent, \"WOLFE\", result)\n",
    "\n",
    "test_plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775163f24ba39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import GradientDecent\n",
    "from graphics_plotter import GraphicsPlotter\n",
    "from lrs import linear_search\n",
    "from linear_algo import dichotomy\n",
    "import math\n",
    "\n",
    "func = FunctionWrapper(lambda x: x[0] ** 2 + 3 * x[1] ** 2 + x[0] * x[1] - 1)\n",
    "\n",
    "linear_search = GradientDecent(linear_search(0.0001, 1000, dichotomy), func, [[-6.0, 6.0], [-6.0, 6.0]], 0.00001)\n",
    "test_painter = GraphicsPlotter(linear_search)\n",
    "func.clear()\n",
    "gradient.clear()\n",
    "res = linear_search.find_min([3, 0], 1000)\n",
    "\n",
    "pretty_print(linear_search, \"LIN_GRAD\", res)\n",
    "\n",
    "test_painter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec9fa3-da3b-4ec3-9144-cd70671e208d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
